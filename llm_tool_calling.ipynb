{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKr7dEEFzyK7401BkMak4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariomshahu/LLM/blob/main/llm_tool_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "from huggingface_hub import login\n",
        "import json"
      ],
      "metadata": {
        "id": "wYfEDjC5uYES"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq')"
      ],
      "metadata": {
        "id": "oYNOopDL3vGo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust LLM JSON Output Validation and Prompt Engineering"
      ],
      "metadata": {
        "id": "LIMyq_l_AxoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class WeatherQuery(BaseModel):\n",
        "    location: str = Field(description=\"City to check weather for\")\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"groq\",\n",
        "    api_key=groq_api_key\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Respond only with a JSON object for the weather query.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Weather in Paris?\"}\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "    messages=messages\n",
        ")\n",
        "print(response.choices[0].message[\"content\"])\n",
        "\n",
        "weather_query = WeatherQuery.model_validate_json(response.choices[0].message[\"content\"])\n",
        "print(weather_query.location)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "XLrsvUdCA5eq",
        "outputId": "37c15425-74e1-4d6f-a293-6e4a025e590e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"city\": \"Paris\",\n",
            "  \"country\": \"France\",\n",
            "  \"temperature\": 22,\n",
            "  \"conditions\": \"Partly Cloudy\",\n",
            "  \"humidity\": 60,\n",
            "  \"wind_speed\": 15\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for WeatherQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"city\": \"P...wind_speed\": 15\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1760520561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Suppose the output is '{\"location\": \"Paris\"}', you then use Pydantic to parse/validate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mweather_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeatherQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_validate_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36mmodel_validate_json\u001b[0;34m(cls, json_data, strict, context, by_alias, by_name)\u001b[0m\n\u001b[1;32m    744\u001b[0m             )\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         return cls.__pydantic_validator__.validate_json(\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_alias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_alias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         )\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for WeatherQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"city\": \"P...wind_speed\": 15\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from huggingface_hub import InferenceClient\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "class WeatherQuery(BaseModel):\n",
        "    location: str = Field(description=\"City to check weather for\")\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"groq\",\n",
        "    api_key=groq_api_key\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": (\n",
        "            \"Respond only with a minified JSON object matching this schema: \"\n",
        "            '{\"location\": \"<city name as string>\"}. '\n",
        "            \"Do not include any Markdown formatting, code block markers, explanations, or extra text.\"\n",
        "        )},\n",
        "    {\"role\": \"user\", \"content\": \"Weather in Paris?\"}\n",
        "]\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "def query_with_retry(messages, max_retries=3, delay=2):\n",
        "    last_exception = None\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "                messages=messages\n",
        "            )\n",
        "            output = response.choices[0].message[\"content\"]\n",
        "            # Attempt to parse and validate JSON output\n",
        "            weather_query = WeatherQuery.model_validate_json(output)\n",
        "            return weather_query\n",
        "        except (json.JSONDecodeError, ValidationError) as e:\n",
        "            print(f\"Attempt {attempt}: Failed to parse/validate response. Error: {e}\")\n",
        "            last_exception = e\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(delay)  # Wait before retrying\n",
        "    raise RuntimeError(f\"All {max_retries} attempts failed. Last error: {last_exception}\")\n",
        "\n",
        "try:\n",
        "    weather_query = query_with_retry(messages, MAX_RETRIES)\n",
        "    print(weather_query.location)\n",
        "except RuntimeError as err:\n",
        "    print(f\"Error: {err}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZQrXVV1MeWA",
        "outputId": "ec284fdb-f661-449a-a1d0-599b63e8f573"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from huggingface_hub import InferenceClient\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "class WeatherQuery(BaseModel):\n",
        "    location: str = Field(description=\"City to check weather for\")\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"groq\",\n",
        "    api_key=groq_api_key\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Respond only with a JSON object for the weather query.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Weather in Paris?\"}\n",
        "]\n",
        "\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "def query_with_retry(messages, max_retries=3, delay=2):\n",
        "    last_exception = None\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "                messages=messages\n",
        "            )\n",
        "            output = response.choices[0].message[\"content\"]\n",
        "            weather_query = WeatherQuery.model_validate_json(output)\n",
        "            return weather_query\n",
        "        except (json.JSONDecodeError, ValidationError) as e:\n",
        "            print(f\"Attempt {attempt}: Failed to parse/validate response. Error: {e}\")\n",
        "            last_exception = e\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(delay)\n",
        "    raise RuntimeError(f\"All {max_retries} attempts failed. Last error: {last_exception}\")\n",
        "\n",
        "try:\n",
        "    weather_query = query_with_retry(messages, MAX_RETRIES)\n",
        "    print(weather_query.location)\n",
        "except RuntimeError as err:\n",
        "    print(f\"Error: {err}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkbAOpOBOv1o",
        "outputId": "d61ab535-f14f-4b11-d83e-a1f77f884a51"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt 1: Failed to parse/validate response. Error: 1 validation error for WeatherQuery\n",
            "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"city\": \"P...wind_speed\": 15\\n}\\n```', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "Attempt 2: Failed to parse/validate response. Error: 1 validation error for WeatherQuery\n",
            "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"temperatu...wind_speed\": 15\\n}\\n```', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "Attempt 3: Failed to parse/validate response. Error: 1 validation error for WeatherQuery\n",
            "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"city\": \"P...wind_speed\": 15\\n}\\n```', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "Error: All 3 attempts failed. Last error: 1 validation error for WeatherQuery\n",
            "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"city\": \"P...wind_speed\": 15\\n}\\n```', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p6y2soNpPJOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three-Stage Retry Mechanism When Using Tool Calls"
      ],
      "metadata": {
        "id": "Uls3U5ywVhwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(\n",
        "    provider=\"groq\",\n",
        "    api_key=groq_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "BrhMfcuGZyVF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import inspect\n",
        "import time\n",
        "import random\n",
        "from typing import Dict, Any, Callable, Optional, List\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "class RetryStage(Enum):\n",
        "    \"\"\"Three distinct retry stages\"\"\"\n",
        "    LLM_API_CALL = \"llm_api_call\"\n",
        "    TOOL_API_CALL = \"tool_api_call\"\n",
        "    JSON_VALIDATION = \"json_validation\"\n",
        "\n",
        "class RetryableError(Enum):\n",
        "    \"\"\"Types of errors that should trigger retries\"\"\"\n",
        "    # Stage 1: LLM API Call errors\n",
        "    LLM_NETWORK_ERROR = \"llm_network_error\"\n",
        "    LLM_TIMEOUT_ERROR = \"llm_timeout_error\"\n",
        "    LLM_RATE_LIMIT_ERROR = \"llm_rate_limit_error\"\n",
        "    LLM_API_ERROR = \"llm_api_error\"\n",
        "\n",
        "    # Stage 2: Tool API Call errors\n",
        "    TOOL_NETWORK_ERROR = \"tool_network_error\"\n",
        "    TOOL_TIMEOUT_ERROR = \"tool_timeout_error\"\n",
        "    TOOL_RATE_LIMIT_ERROR = \"tool_rate_limit_error\"\n",
        "    TOOL_API_ERROR = \"tool_api_error\"\n",
        "\n",
        "    # Stage 3: JSON Validation errors\n",
        "    MALFORMED_JSON = \"malformed_json\"\n",
        "    INVALID_PARAMETERS = \"invalid_parameters\"\n",
        "    MISSING_REQUIRED_PARAMS = \"missing_required_params\"\n",
        "    TYPE_MISMATCH = \"type_mismatch\"\n",
        "\n",
        "@dataclass\n",
        "class StageRetryConfig:\n",
        "    \"\"\"Configuration for each retry stage\"\"\"\n",
        "    max_retries: int = 3\n",
        "    base_delay: float = 1.0\n",
        "    max_delay: float = 60.0\n",
        "    backoff_multiplier: float = 2.0\n",
        "    jitter: bool = True\n",
        "    retryable_errors: List[RetryableError] = None\n",
        "    temperature_adjustment: float = 0.0  # For LLM retries\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.retryable_errors is None:\n",
        "            self.retryable_errors = []\n",
        "\n",
        "@dataclass\n",
        "class ThreeStageRetryConfig:\n",
        "    \"\"\"Configuration for all three retry stages\"\"\"\n",
        "    # Stage 1: LLM API Call retries\n",
        "    llm_api_retry: StageRetryConfig = None\n",
        "\n",
        "    # Stage 2: Tool API Call retries\n",
        "    tool_api_retry: StageRetryConfig = None\n",
        "\n",
        "    # Stage 3: JSON Validation retries\n",
        "    json_validation_retry: StageRetryConfig = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.llm_api_retry is None:\n",
        "            self.llm_api_retry = StageRetryConfig(\n",
        "                max_retries=3,\n",
        "                base_delay=1.0,\n",
        "                backoff_multiplier=2.0,\n",
        "                temperature_adjustment=0.1,\n",
        "                retryable_errors=[\n",
        "                    RetryableError.LLM_NETWORK_ERROR,\n",
        "                    RetryableError.LLM_TIMEOUT_ERROR,\n",
        "                    RetryableError.LLM_RATE_LIMIT_ERROR,\n",
        "                    RetryableError.LLM_API_ERROR\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        if self.tool_api_retry is None:\n",
        "            self.tool_api_retry = StageRetryConfig(\n",
        "                max_retries=3,\n",
        "                base_delay=0.5,\n",
        "                backoff_multiplier=1.5,\n",
        "                retryable_errors=[\n",
        "                    RetryableError.TOOL_NETWORK_ERROR,\n",
        "                    RetryableError.TOOL_TIMEOUT_ERROR,\n",
        "                    RetryableError.TOOL_RATE_LIMIT_ERROR,\n",
        "                    RetryableError.TOOL_API_ERROR\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        if self.json_validation_retry is None:\n",
        "            self.json_validation_retry = StageRetryConfig(\n",
        "                max_retries=2,\n",
        "                base_delay=0.3,\n",
        "                backoff_multiplier=1.2,\n",
        "                temperature_adjustment=0.15,\n",
        "                retryable_errors=[\n",
        "                    RetryableError.MALFORMED_JSON,\n",
        "                    RetryableError.INVALID_PARAMETERS,\n",
        "                    RetryableError.MISSING_REQUIRED_PARAMS,\n",
        "                    RetryableError.TYPE_MISMATCH\n",
        "                ]\n",
        "            )\n",
        "\n",
        "def calculate_delay(attempt: int, config: StageRetryConfig) -> float:\n",
        "    delay = config.base_delay * (config.backoff_multiplier ** attempt)\n",
        "    delay = min(delay, config.max_delay)\n",
        "\n",
        "    if config.jitter:\n",
        "        jitter_range = delay * 0.25\n",
        "        delay += random.uniform(-jitter_range, jitter_range)\n",
        "\n",
        "    return max(0, delay)\n",
        "\n",
        "def classify_llm_api_error(error: Exception, error_msg: str) -> Optional[RetryableError]:\n",
        "    error_msg_lower = error_msg.lower()\n",
        "\n",
        "    if any(keyword in error_msg_lower for keyword in ['connection', 'network', 'dns', 'socket']):\n",
        "        return RetryableError.LLM_NETWORK_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['timeout', 'timed out']):\n",
        "        return RetryableError.LLM_TIMEOUT_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['rate limit', 'too many requests', '429']):\n",
        "        return RetryableError.LLM_RATE_LIMIT_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['api', 'service', '503', '500']):\n",
        "        return RetryableError.LLM_API_ERROR\n",
        "\n",
        "    return None\n",
        "\n",
        "def classify_tool_api_error(error: Exception, error_msg: str) -> Optional[RetryableError]:\n",
        "    error_msg_lower = error_msg.lower()\n",
        "\n",
        "    if any(keyword in error_msg_lower for keyword in ['connection', 'network', 'dns', 'socket']):\n",
        "        return RetryableError.TOOL_NETWORK_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['timeout', 'timed out']):\n",
        "        return RetryableError.TOOL_TIMEOUT_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['rate limit', 'too many requests', '429']):\n",
        "        return RetryableError.TOOL_RATE_LIMIT_ERROR\n",
        "    if any(keyword in error_msg_lower for keyword in ['api', 'service', '503', '500']):\n",
        "        return RetryableError.TOOL_API_ERROR\n",
        "\n",
        "    return None\n",
        "\n",
        "def classify_json_validation_error(error: Exception, error_msg: str) -> Optional[RetryableError]:\n",
        "    error_msg_lower = error_msg.lower()\n",
        "\n",
        "    if any(keyword in error_msg_lower for keyword in ['json', 'decode', 'parse']):\n",
        "        return RetryableError.MALFORMED_JSON\n",
        "    if any(keyword in error_msg_lower for keyword in ['missing', 'required']):\n",
        "        return RetryableError.MISSING_REQUIRED_PARAMS\n",
        "    if any(keyword in error_msg_lower for keyword in ['invalid', 'unexpected']):\n",
        "        return RetryableError.INVALID_PARAMETERS\n",
        "    if any(keyword in error_msg_lower for keyword in ['type', 'mismatch', 'wrong type']):\n",
        "        return RetryableError.TYPE_MISMATCH\n",
        "\n",
        "    return None\n",
        "\n",
        "# weather tools (with simulated failures for testing)\n",
        "def get_temperature(location: str):\n",
        "    \"\"\"Get the temperature for a given location\"\"\"\n",
        "    # Simulate occasional tool API failures (Stage 2)\n",
        "    if random.random() < 0.15:\n",
        "        raise ConnectionError(\"Weather API connection error\")\n",
        "\n",
        "    temperatures = {\"New York\": \"22°C\", \"London\": \"18°C\", \"Tokyo\": \"26°C\", \"Sydney\": \"20°C\"}\n",
        "    return temperatures.get(location, \"Temperature data not available\")\n",
        "\n",
        "def get_weather_condition(location: str):\n",
        "    \"\"\"Get the weather condition for a given location\"\"\"\n",
        "    # Simulate occasional tool API failures (Stage 2)\n",
        "    if random.random() < 0.15:\n",
        "        raise TimeoutError(\"Weather API timeout\")\n",
        "\n",
        "    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n",
        "    return conditions.get(location, \"Weather condition data not available\")\n",
        "\n",
        "def stage1_llm_api_call_with_retry(client, model, messages, tools, tool_choice, max_tokens, temperature, retry_config):\n",
        "    \"\"\"\n",
        "    Stage 1: LLM API Call with retry mechanism\n",
        "    Handles LLM service errors, network issues, rate limits\n",
        "    \"\"\"\n",
        "    print(f\"\\n STAGE 1: LLM API Call\")\n",
        "\n",
        "    last_error = None\n",
        "    retry_history = []\n",
        "    current_temperature = temperature\n",
        "\n",
        "    for attempt in range(retry_config.max_retries + 1):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{retry_config.max_retries + 1} (temp: {current_temperature:.1f})\")\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                tools=tools,\n",
        "                tool_choice=tool_choice,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=current_temperature\n",
        "            )\n",
        "\n",
        "            if retry_history:\n",
        "                print(f\"   LLM API succeeded after {attempt} retries\")\n",
        "            else:\n",
        "                print(f\"   LLM API succeeded on first attempt\")\n",
        "\n",
        "            return {\"success\": True, \"response\": response, \"retry_history\": retry_history}\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            error_msg = str(e)\n",
        "\n",
        "            retryable_error_type = classify_llm_api_error(e, error_msg)\n",
        "            is_retryable = (\n",
        "                retryable_error_type is not None and\n",
        "                retryable_error_type in retry_config.retryable_errors and\n",
        "                attempt < retry_config.max_retries\n",
        "            )\n",
        "\n",
        "            retry_info = {\n",
        "                \"stage\": \"llm_api_call\",\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"error\": error_msg,\n",
        "                \"error_type\": type(e).__name__,\n",
        "                \"retryable_error_type\": retryable_error_type.value if retryable_error_type else None,\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            retry_history.append(retry_info)\n",
        "\n",
        "            if is_retryable:\n",
        "                delay = calculate_delay(attempt, retry_config)\n",
        "                print(f\"   LLM API failed: {error_msg}\")\n",
        "                print(f\"   Retrying in {delay:.2f}s...\")\n",
        "\n",
        "                retry_info[\"delay\"] = delay\n",
        "                time.sleep(delay)\n",
        "\n",
        "                # Adjust temperature for different responses\n",
        "                current_temperature = min(current_temperature + retry_config.temperature_adjustment, 0.9)\n",
        "            else:\n",
        "                print(f\"   LLM API failed (non-retryable): {error_msg}\")\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"success\": False,\n",
        "        \"error\": f\"LLM API failed: {str(last_error)}\",\n",
        "        \"retry_history\": retry_history\n",
        "    }\n",
        "\n",
        "def stage3_json_validation_with_retry(tool_call, available_functions, retry_config, client, model, messages, tools, tool_choice, max_tokens, base_temperature):\n",
        "    \"\"\"\n",
        "    Stage 3: JSON Validation with retry mechanism\n",
        "    Handles malformed JSON, wrong parameters, type mismatches\n",
        "    \"\"\"\n",
        "    print(f\"\\n STAGE 3: JSON Validation for {tool_call.function.name}\")\n",
        "\n",
        "    last_error = None\n",
        "    retry_history = []\n",
        "    current_temperature = base_temperature\n",
        "    current_tool_call = tool_call\n",
        "\n",
        "    for attempt in range(retry_config.max_retries + 1):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{retry_config.max_retries + 1}\")\n",
        "\n",
        "            function_name = current_tool_call.function.name\n",
        "\n",
        "            if function_name not in available_functions:\n",
        "                raise ValueError(f\"Function '{function_name}' not found in available functions\")\n",
        "\n",
        "            function_to_call = available_functions[function_name]\n",
        "\n",
        "            try:\n",
        "                function_args = json.loads(current_tool_call.function.arguments)\n",
        "            except json.JSONDecodeError as e:\n",
        "                raise ValueError(f\"Malformed JSON in function arguments: {str(e)}\")\n",
        "\n",
        "            sig = inspect.signature(function_to_call)\n",
        "            expected_params = list(sig.parameters.keys())\n",
        "\n",
        "            required_params = [\n",
        "                name for name, param in sig.parameters.items()\n",
        "                if param.default == inspect.Parameter.empty\n",
        "            ]\n",
        "\n",
        "            missing_params = [param for param in required_params if param not in function_args]\n",
        "            if missing_params:\n",
        "                raise ValueError(f\"Missing required parameters: {missing_params}\")\n",
        "\n",
        "            unexpected_params = [k for k in function_args.keys() if k not in expected_params]\n",
        "            if unexpected_params:\n",
        "                raise ValueError(f\"Invalid parameters: {unexpected_params}\")\n",
        "\n",
        "            valid_args = {k: v for k, v in function_args.items() if k in expected_params}\n",
        "\n",
        "            if retry_history:\n",
        "                print(f\"   JSON validation succeeded after {attempt} retries\")\n",
        "            else:\n",
        "                print(f\"   JSON validation succeeded on first attempt\")\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"function_to_call\": function_to_call,\n",
        "                \"valid_args\": valid_args,\n",
        "                \"retry_history\": retry_history\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            error_msg = str(e)\n",
        "\n",
        "            retryable_error_type = classify_json_validation_error(e, error_msg)\n",
        "            is_retryable = (\n",
        "                retryable_error_type is not None and\n",
        "                retryable_error_type in retry_config.retryable_errors and\n",
        "                attempt < retry_config.max_retries\n",
        "            )\n",
        "\n",
        "            retry_info = {\n",
        "                \"stage\": \"json_validation\",\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"error\": error_msg,\n",
        "                \"error_type\": type(e).__name__,\n",
        "                \"retryable_error_type\": retryable_error_type.value if retryable_error_type else None,\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            retry_history.append(retry_info)\n",
        "\n",
        "            if is_retryable:\n",
        "                delay = calculate_delay(attempt, retry_config)\n",
        "                print(f\"   JSON validation failed: {error_msg}\")\n",
        "                print(f\"   Re-calling LLM in {delay:.2f}s...\")\n",
        "\n",
        "                retry_info[\"delay\"] = delay\n",
        "                time.sleep(delay)\n",
        "\n",
        "                # Increase temperature and retry LLM call to get different JSON\n",
        "                current_temperature = min(current_temperature + retry_config.temperature_adjustment, 0.9)\n",
        "\n",
        "                # Make new LLM call to get corrected JSON\n",
        "                llm_result = stage1_llm_api_call_with_retry(\n",
        "                    client, model, messages[:1] + [{\"role\": \"user\", \"content\": f\"Please provide the correct tool call for {function_name}. The previous attempt had this error: {error_msg}\"}],\n",
        "                    tools, tool_choice, max_tokens, current_temperature,\n",
        "                    StageRetryConfig(max_retries=1, base_delay=0.5)\n",
        "                )\n",
        "\n",
        "                if llm_result[\"success\"]:\n",
        "                    new_response = llm_result[\"response\"]\n",
        "                    if hasattr(new_response.choices[0].message, 'tool_calls') and new_response.choices[0].message.tool_calls:\n",
        "                        current_tool_call = new_response.choices[0].message.tool_calls[0]\n",
        "                        print(f\"   Got new tool call, retrying validation...\")\n",
        "                    else:\n",
        "                        print(f\"   LLM didn't provide tool calls in retry\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(f\"   Failed to get new LLM response\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"   JSON validation failed (non-retryable): {error_msg}\")\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"success\": False,\n",
        "        \"error\": f\"JSON validation failed: {str(last_error)}\",\n",
        "        \"retry_history\": retry_history\n",
        "    }\n",
        "\n",
        "def stage2_tool_api_call_with_retry(function_to_call, function_args, function_name, retry_config):\n",
        "    \"\"\"\n",
        "    Stage 2: Tool API Call with retry mechanism\n",
        "    Handles tool execution errors, network issues, API failures\n",
        "    \"\"\"\n",
        "    print(f\"\\n STAGE 2: Tool API Call for {function_name}\")\n",
        "\n",
        "    last_error = None\n",
        "    retry_history = []\n",
        "\n",
        "    for attempt in range(retry_config.max_retries + 1):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{retry_config.max_retries + 1}\")\n",
        "\n",
        "            result = function_to_call(**function_args)\n",
        "\n",
        "            if retry_history:\n",
        "                print(f\"   Tool API succeeded after {attempt} retries\")\n",
        "            else:\n",
        "                print(f\"   Tool API succeeded on first attempt\")\n",
        "\n",
        "            return {\"success\": True, \"result\": result, \"retry_history\": retry_history}\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            error_msg = str(e)\n",
        "\n",
        "            retryable_error_type = classify_tool_api_error(e, error_msg)\n",
        "            is_retryable = (\n",
        "                retryable_error_type is not None and\n",
        "                retryable_error_type in retry_config.retryable_errors and\n",
        "                attempt < retry_config.max_retries\n",
        "            )\n",
        "\n",
        "            retry_info = {\n",
        "                \"stage\": \"tool_api_call\",\n",
        "                \"attempt\": attempt + 1,\n",
        "                \"error\": error_msg,\n",
        "                \"error_type\": type(e).__name__,\n",
        "                \"retryable_error_type\": retryable_error_type.value if retryable_error_type else None,\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            retry_history.append(retry_info)\n",
        "\n",
        "            if is_retryable:\n",
        "                delay = calculate_delay(attempt, retry_config)\n",
        "                print(f\"   Tool API failed: {error_msg}\")\n",
        "                print(f\"   Retrying in {delay:.2f}s...\")\n",
        "\n",
        "                retry_info[\"delay\"] = delay\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"   Tool API failed (non-retryable): {error_msg}\")\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"success\": False,\n",
        "        \"error\": f\"Tool API failed: {str(last_error)}\",\n",
        "        \"retry_history\": retry_history\n",
        "    }\n",
        "\n",
        "def three_stage_tool_execution(tool_call, available_functions, retry_config, client, model, messages, tools, tool_choice, max_tokens, base_temperature):\n",
        "\n",
        "    print(f\"\\n Processing tool call: {tool_call.function.name}\")\n",
        "\n",
        "    all_retry_history = []\n",
        "\n",
        "    # Stage 3: JSON Validation\n",
        "    validation_result = stage3_json_validation_with_retry(\n",
        "        tool_call, available_functions, retry_config.json_validation_retry,\n",
        "        client, model, messages, tools, tool_choice, max_tokens, base_temperature\n",
        "    )\n",
        "\n",
        "    if validation_result[\"retry_history\"]:\n",
        "        all_retry_history.extend(validation_result[\"retry_history\"])\n",
        "\n",
        "    if not validation_result[\"success\"]:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": validation_result[\"error\"],\n",
        "            \"retry_history\": all_retry_history,\n",
        "            \"failed_stage\": \"json_validation\"\n",
        "        }\n",
        "\n",
        "    # Stage 2: Tool API Call\n",
        "    execution_result = stage2_tool_api_call_with_retry(\n",
        "        validation_result[\"function_to_call\"],\n",
        "        validation_result[\"valid_args\"],\n",
        "        tool_call.function.name,\n",
        "        retry_config.tool_api_retry\n",
        "    )\n",
        "\n",
        "    if execution_result[\"retry_history\"]:\n",
        "        all_retry_history.extend(execution_result[\"retry_history\"])\n",
        "\n",
        "    if not execution_result[\"success\"]:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": execution_result[\"error\"],\n",
        "            \"retry_history\": all_retry_history,\n",
        "            \"failed_stage\": \"tool_api_call\"\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"success\": True,\n",
        "        \"result\": execution_result[\"result\"],\n",
        "        \"retry_history\": all_retry_history\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize retry configuration\n",
        "    retry_config = ThreeStageRetryConfig()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather and temperature like in New York and London? Respond with one sentence for each city. Use tools to get the information.\"},\n",
        "    ]\n",
        "\n",
        "    tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_temperature\",\n",
        "                \"description\": \"Get the temperature for a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The name of the city\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_weather_condition\",\n",
        "                \"description\": \"Get the weather condition for a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The name of the city\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    available_functions = {\n",
        "        \"get_temperature\": get_temperature,\n",
        "        \"get_weather_condition\": get_weather_condition,\n",
        "    }\n",
        "\n",
        "    # Stage 1: Initial LLM API call\n",
        "    llm_result = stage1_llm_api_call_with_retry(\n",
        "        client=client,\n",
        "        model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=4096,\n",
        "        temperature=0.5,\n",
        "        retry_config=retry_config.llm_api_retry\n",
        "    )\n",
        "\n",
        "    if not llm_result[\"success\"]:\n",
        "        print(f\" FAILED: {llm_result['error']}\")\n",
        "        exit(1)\n",
        "\n",
        "    response = llm_result[\"response\"]\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    # Process tool calls\n",
        "    messages.append(response_message)\n",
        "\n",
        "    for tool_call in tool_calls:\n",
        "        execution_result = three_stage_tool_execution(\n",
        "            tool_call, available_functions, retry_config,\n",
        "            client, \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "            messages, tools, \"auto\", 4096, 0.5\n",
        "        )\n",
        "\n",
        "        if execution_result[\"success\"]:\n",
        "            function_response = execution_result[\"result\"]\n",
        "            retry_summary = \"\"\n",
        "            if execution_result[\"retry_history\"]:\n",
        "                total_retries = len(execution_result[\"retry_history\"])\n",
        "                retry_summary = f\" (succeeded after {total_retries} total retries across stages)\"\n",
        "            print(f\" Overall success for {tool_call.function.name}{retry_summary}: {function_response}\")\n",
        "        else:\n",
        "            error_msg = execution_result[\"error\"]\n",
        "            failed_stage = execution_result.get(\"failed_stage\", \"unknown\")\n",
        "            retry_summary = \"\"\n",
        "            if execution_result[\"retry_history\"]:\n",
        "                total_retries = len(execution_result[\"retry_history\"])\n",
        "                retry_summary = f\" (failed after {total_retries} total retries, failed at {failed_stage} stage)\"\n",
        "            print(f\" Overall failure for {tool_call.function.name}{retry_summary}: {error_msg}\")\n",
        "            function_response = f\"Error: {error_msg}\"\n",
        "\n",
        "        # Add tool response to messages\n",
        "        messages.append({\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": str(function_response),\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "        })\n",
        "\n",
        "    # Stage 1: Final LLM API call\n",
        "    print(f\"\\n FINAL LLM CALL\")\n",
        "    final_llm_result = stage1_llm_api_call_with_retry(\n",
        "        client=client,\n",
        "        model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=4096,\n",
        "        temperature=0.5,\n",
        "        retry_config=retry_config.llm_api_retry\n",
        "    )\n",
        "\n",
        "    if final_llm_result[\"success\"]:\n",
        "        final_response = final_llm_result[\"response\"]\n",
        "        print(f\"\\n FINAL RESPONSE:\")\n",
        "        print(final_response.choices[0].message.content)\n",
        "    else:\n",
        "        print(f\" FINAL LLM CALL FAILED: {final_llm_result['error']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqVORWflVJKf",
        "outputId": "120c9431-9ae2-48e7-89d3-f0e56df69fe8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " STAGE 1: LLM API Call\n",
            "  Attempt 1/4 (temp: 0.5)\n",
            "   LLM API succeeded on first attempt\n",
            "\n",
            " Processing tool call: get_weather_condition\n",
            "\n",
            " STAGE 3: JSON Validation for get_weather_condition\n",
            "  Attempt 1/3\n",
            "   JSON validation succeeded on first attempt\n",
            "\n",
            " STAGE 2: Tool API Call for get_weather_condition\n",
            "  Attempt 1/4\n",
            "   Tool API succeeded on first attempt\n",
            " Overall success for get_weather_condition: Sunny\n",
            "\n",
            " Processing tool call: get_temperature\n",
            "\n",
            " STAGE 3: JSON Validation for get_temperature\n",
            "  Attempt 1/3\n",
            "   JSON validation succeeded on first attempt\n",
            "\n",
            " STAGE 2: Tool API Call for get_temperature\n",
            "  Attempt 1/4\n",
            "   Tool API succeeded on first attempt\n",
            " Overall success for get_temperature: 22°C\n",
            "\n",
            " FINAL LLM CALL\n",
            "\n",
            " STAGE 1: LLM API Call\n",
            "  Attempt 1/4 (temp: 0.5)\n",
            "   LLM API succeeded on first attempt\n",
            "\n",
            " FINAL RESPONSE:\n",
            "The weather in New York is sunny with a temperature of 22°C.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}